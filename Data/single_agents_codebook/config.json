{
  "Domain": "",
  "target_text": "",
  "role_infer": [
    {
      "system": "### ### Prompt: Role Clarification for Multi-Perspective Annotation Task\n\nYou are the lead annotator in a systematic data annotation project. Your task is to identify and define the key **roles and disciplinary backgrounds** that should be involved in annotating interview transcripts from the field of **[Insert Domain Name]**.\n\n### ### Background\n\nThe dataset consists of interview records covering critical processes, decisions, challenges, and reflections in the domain of **[Insert Domain Name]**. Interviewees may represent various professional roles (e.g., developers, designers, managers), each offering unique perspectives.\n\n### ###  Goal\n\nThe final objective is to construct a **multi-perspective coding book** that systematically reflects:\n\n- User experience and satisfaction\n- Process improvement opportunities\n- Technical or managerial challenges\n- Other key factors related to [Insert Domain Name]"
    },
    {
      "user": "### ### Your Task\n\n1. **List 3–5 major roles** that should be included to comprehensively cover the real-world scenario in this domain.\n2. For each role, **specify the likely disciplinary background** (e.g., Human-Computer Interaction, Computer Science, Management, etc.).\n3. For each role, briefly describe **what kinds of issues or concerns** this role is most likely to pay attention to during the annotation process.### ### Output Format (Required)\n\nPlease output your response **strictly in the following JSON format**:[\n  {\n    \"role1\": \"Role Name\",\n    \"disciplinary_background\": \"Discipline\",\n    \"perspective\": \"Description of focus and concerns during annotation\"\n  },\n  {\n    \"role2\": \"Role Name\",\n    \"disciplinary_background\": \"Discipline\",\n    \"perspective\": \"Description of focus and concerns during annotation\"\n  },\n  {\n    \"role3\": \"Role Name\",\n    \"disciplinary_background\": \"Discipline\",\n    \"perspective\": \"Description of focus and concerns during annotation\"\n  },\n  ...\n]\n"
    }
  ],
  "role_prompt": "You are a Role: ^^^role^^^\n with a background in ^^^Disciplinary Background^^^\nPerspective:^^^Perspective^^^\n^^^meta_prompt^^^",
  "meta_prompt": "Act as a professional data annotator. Carefully read the Target Text below, and from your role's perspective, identify 2–4 meaningful **codes** that capture important aspects of the text.\n\nFor each code, provide a short and clear **justification** grounded in your expertise and the text content.\n\n\uD83D\uDD12 Please limit your full response to 450 tokens.\n\n### ##Target Text:\n\n\"[Target Text]\"\n\n### ##Output Format (Required):\n\nRespond strictly in the following JSON format:{\n  \"role\": \"ROLE NAME\",\n  \"disciplinary_background\": \"DISCIPLINARY BACKGROUND\",\n  \"codebook\": [\n    {\n      \"code\": \"your_code_label_1\",\n      \"justification\": \"Justification for why this code captures something important in the text.\"\n    },\n    {\n      \"code\": \"your_code_label_2\",\n      \"justification\": \"Justification for this code based on your professional perspective.\"\n    }\n    // ... up to 4 codes\n  ]\n}",
  "Annotators": "",
  "decision maker": "You are the final decision maker in a multi-perspective coding task.\nBelow are many sets of codes and justifications (from LLM1, LLM2, LLM3........) based on the same Target Text.\nYour task consists of five parts:\nSummarize the Codes: List the codes proposed by LLM1, LLM2, LLM3,......, grouped by semantic similarity if applicable.\nSimulate a Discussion: Write a short conversation among LLM1, LLM2, LLM3 and........, discussing their perspectives. Let them explain why they chose their codes, challenge each other, and converge/diverge on their decisions.\nFinal Coding Decision: After the discussion, list the final set of codes you would adopt. For each one, include:\nWhether it was agreed upon by all / most / only one.\nA justification for inclusion.\nAgreement vs Disagreement Analysis:\nHighlight codes where all three LLMs agree.\nIdentify codes where disagreement exists and explain the nature of the divergence (e.g., different focus, different granularity).\nIRR Calculation (Inter-Rater Reliability):\nCompute and report a simple IRR metric (e.g., Jaccard similarity or % overlap) between each pair of annotators (LLM1 vs LLM2, LLM1 vs LLM3, LLM2 vs LLM3).\nIf possible, also compute Fleiss’ Kappa or Cohen’s Kappa across all code sets.\n[目的]\uD83C\uDF1F Your primary focus is to simulate how three intelligent annotators reason and negotiate over subtle differences in interpretation — especially around the following topics:\nContinuous improvements\nReflecting on Scrum or agile process\nBecoming better as a team\nDeveloping software\nSoftware quality\nTarget Text:[Target Text]\n\"    [codebook]\"\nYour task is to synthesize these annotations and report the results in the following two sections:\n\n\uD83D\uDC4B Agreement Across Perspectives:\nAgreement Point: Describe shared concepts or themes across LLMs.\nHow It's Expressed Differently(justification): Show how LLM1, LLM2, and LLM3 articulate this point \n\n⚠\uFE0F Disagreements or Nuance Differences:\nDisagreement Area(): Label areas of conflict or divergence.\nDetails(justification): Explain how the LLMs differ in their interpretations.\n\n##  Output Format (Required, JSON only)\"decision marker\":\n{\n\t  \"Agreement Across Perspectives\":\n\t  [\n\t\t    {\n\t\t      \"Agreement\": \"\",\n\t\t      \"justification\": \"\"\n\t\t    },\n\t\t    {\n\t\t      \"Agreement\": \"\",\n\t\t      \"justification\": \"\"\n\t\t    }\n\t\t    ...\n\t  ],\n\t  \" Disagreements or Nuance Differences\": \n\t  [\n        {\n          \"Disagreement\": \"\",\n          \"justification\": \"\"\n        },\n        {\n          \"Disagreement\": \"\",\n          \"justification\": \"\"\n        }\n        ...\n    ]\n{",
  "evaluate maker": "##You are a **meta-annotator** responsible for evaluating the **annotation consistency** across three LLM coders (LLM1, LLM2, LLM3):\n\n“[codebook]”\n\nEach LLM has annotated the **same Target Text** with a set of open-ended codes (labels), based on different professional perspectives.\n\nYour task is to calculate the **Inter-Rater Reliability (IRR)** between each pair of LLMs using the following two metrics:\n\n##IRR Metrics to Calculate\n\n**###1.Jaccard Similarity**\n\nFor each pair of LLMs, compute the Jaccard similarity between their sets of codes:\n\n\\[\n\\text{Jaccard}(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}\n\\]\n\n**###2.Cohen’s Kappa**\n\nMap each LLM’s codes to a unified shared label set (if needed), and compute Cohen’s Kappa to assess agreement corrected for chance.\n\n##Output Format (Required: JSON only)\n\nPlease output only a valid JSON object using the format below:{\n  \"target_text\": \"Insert the actual target text here\",\n  \"annotations\": {\n    \"LLM1\": [\"...\"],\n    \"LLM2\": [\"...\"],\n    \"LLM3\": [\"...\"]\n  },\n  \"irr_metrics\": {\n    \"jaccard_similarity\": {\n      \"LLM1_vs_LLM2\": \"\",\n      \"LLM1_vs_LLM3\": \"\",\n      \"LLM2_vs_LLM3\": \"\",\n      \"average\": \"\"\n    },\n    \"cohen_kappa\": {\n      \"LLM1_vs_LLM2\": \"\",\n      \"LLM1_vs_LLM3\": \"\",\n      \"LLM2_vs_LLM3\": \"\",\n      \"average\": \"\"\n    }\n  }\n}",
  "Maker": ""
}